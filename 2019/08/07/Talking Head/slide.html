
<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <title>Few-Shot Adversarial Learning of Realistic Neural Talking Head Models</title>

        <link rel="stylesheet" href="https://cdn.bootcss.com/reveal.js/3.4.1/css/reveal.min.css">
        
        <!-- theme -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            var theme ='black';
            switch (theme){
                case 'black':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/black.min.css';
                    break;
                case 'beige':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/beige.min.css';
                    break;
                case 'blood':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/blood.min.css';
                    break;
                case 'league':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/league.min.css';
                    break;
                case 'moon':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/moon.min.css';
                    break;
                case 'night':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/night.min.css';
                    break;
                case 'serif':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/serif.min.css';
                    break;
                case 'sky':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/sky.min.css';
                    break;
                case 'solarized':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/solarized.min.css';
                    break;
                case 'white':
                    link.href = 'https://cdn.bootcss.com/reveal.js/3.4.1/css/theme/white.min.css';
                    break;
                default:
            }
            
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>
        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="https://cdn.bootcss.com/reveal.js/3.4.1/lib/css/zenburn.min.css">
        <link href="https://cdn.bootcss.com/reveal.js/3.4.1/css/print/paper.min.css" rel="stylesheet">
        <!-- Printing and PDF exports -->
        
    </head>
    <body>
        <div class="reveal">
            <div class="slides">
                <section data-markdown
                         data-separator="^\n---\n"
                         data-separator-vertical="^\n--\n"
                         data-separator-notes="^Note:">
                    <script type="text/template">Few-Shot Adversarial Learning of Realistic Neural Talking Head Models




--



![1565166694515](\Talking Head\1565166694515.png)

<!-- .element style="border: 0; background: None; box-shadow: None" width="2000px"   -->


--













# ABSTRACT

1. Several  recent  works  have  shown  how  highly  realistic human head images can be obtained by training convolutional neural networks to generate them

2.  In order to create a personalized talking head model, these works require

   training  on  a  large  dataset  of  images  of  a  single  person

3. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image

4. Here,we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few-and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators



--

#  Methods

--



1. `$$\hat{\mathbf{e}}_{i}=\frac{1}{K} \sum_{k=1}^{K} E\left(\mathbf{x}_{i}\left(s_{k}\right),\mathbf{y}_{i}\left(s_{k}\right) ; \phi\right)$$` `$$\hat{\mathbf{x}}_{i}(t)=G\left(\mathbf{y}_{i}(t), \hat{\mathbf{e}}_{i} ; \psi, \mathbf{P}\right)$$`
3. `$$\begin{array}{c}{\mathcal{L}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)=\mathcal{L}_{\mathrm{CNT}}(\phi, \psi, \mathbf{P})+} \\ {\mathcal{L}_{\mathrm{ADV}}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)+\mathcal{L}_{\mathrm{MCH}}(\phi, \mathbf{W})}\end{array}$$`
4. `$$\begin{array}{r}{\mathcal{L}_{\mathrm{ADV}}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)=} \\ {\quad-D\left(\hat{\mathbf{x}}_{i}(t), \mathbf{y}_{i}(t), i ; \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)+\mathcal{L}_{\mathrm{FM}}}\end{array}$$`

--

5. `$$\begin{array}{c}{D\left(\hat{\mathbf{x}}_{i}(t), \mathbf{y}_{i}(t), i ; \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)=} \\ {V\left(\hat{\mathbf{x}}_{i}(t), \mathbf{y}_{i}(t) ; \theta\right)^{T}\left(\mathbf{W}_{i}+\mathbf{w}_{0}\right)+b}\end{array}$$`
6. `$$\begin{array}{c}{\mathcal{L}_{\mathrm{DSC}}\left(\phi, \psi, \mathbf{P}, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)=} \\ {\max \left(0,1+D\left(\hat{\mathbf{x}}_{i}(t), \mathbf{y}_{i}(t), i ; \phi, \psi, \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)\right)+} \\ {\max \left(0,1-D\left(\mathbf{x}_{i}(t), \mathbf{y}_{i}(t), i ; \theta, \mathbf{W}, \mathbf{w}_{0}, b\right)\right)}\end{array}$$`



--

---


![1565166768438](\Talking Head\1565166768438.png)

<!-- .element style="border: 0; background: None; box-shadow: None" width="2000px"   -->



---



--





# Few-shotlearningbyﬁne-tuning 

--



1. `$$\hat{\mathbf{e}}_{\mathrm{NEW}}=\frac{1}{T} \sum_{t=1}^{T} E(\mathbf{x}(t), \mathbf{y}(t) ; \phi)$$`
2. `$$\psi^{\prime}=\mathbf{P} \hat{\mathbf{e}}_{\mathrm{NEW}}$$`
3. `$$\begin{array}{c}{D^{\prime}\left(\hat{\mathbf{x}}(t), \mathbf{y}(t) ; \theta, \mathbf{w}^{\prime}, b\right)=} \\ {V(\hat{\mathbf{x}}(t), \mathbf{y}(t) ; \theta)^{T} \mathbf{w}^{\prime}+b}\end{array}$$`
4. `$$\mathbf{w}^{\prime} \text { to the sum of } \mathbf{w}_{0} \text { and } \hat{\mathbf{e}}_{\mathrm{NEW}}$$`
5. `$$\psi^{\prime}=\mathbf{P} \hat{\mathbf{e}}_{\mathrm{NEW}}$$`

--

5. `$$\begin{aligned} \mathcal{L}^{\prime}\left(\psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right) &=\\ \mathcal{L}_{\mathrm{CNT}}^{\prime}\left(\psi, \psi^{\prime}\right) &+\mathcal{L}_{\mathrm{ADV}}^{\prime}\left(\psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right) \end{aligned}$$`
6. `$$\begin{array}{c}{\mathcal{L}_{\mathrm{DSC}}^{\prime}\left(\psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right)=} \\ {\max \left(0,1+D\left(\hat{\mathbf{x}}(t), \mathbf{y}(t) ; \psi, \psi^{\prime}, \theta, \mathbf{w}^{\prime}, b\right)\right)+} \\ {\max \left(0,1-D\left(\mathbf{x}(t), \mathbf{y}(t) ; \theta, \mathbf{w}^{\prime}, b\right)\right)}\end{array}$$`




--


# Experiments



--

1. We then scale up the available dataand train our method on a larger VoxCeleb2 dataset.  Here,we  train  two  variants  of  our  method.   FF  (feed-forward)variant  is  trained  for  150  epochs  without  the  embedding matching  lossLMCHand,  therefore,  we  only  use  it  with-out fine-tuning (by simply predicting adaptive parameters ψ′ via the projection of the embedding ˆeNEW). The FT variant is trained for half as much (75 epochs) but withLMCH,which allows fine-tuning



--


![1565166825921](\Talking Head\1565166825921.png)

<!-- .element style="border: 0; background: None; box-shadow: None" width="2000px"   -->



--



![1565166874552](\Talking Head\1565166874552.png)

<!-- .element style="border: 0; background: None; box-shadow: None" width="2000px"   -->



--



![1565166929345](\Talking Head\1565166929345.png)

<!-- .element style="border: 0; background: None; box-shadow: None" width="2000px"   -->





--





> [ppt]( https://clearlovewl.github.io/2019/08/07/Talking Head/slide.html)

> [ppt]( http://localhost:4000/2019/08/07/Talking Head/slide.html)</script>
                </section>
            </div>
        </div>

        <script src="https://cdn.bootcss.com/reveal.js/3.4.1/lib/js/head.min.js"></script>
        <script src="https://cdn.bootcss.com/reveal.js/3.4.1/js/reveal.min.js"></script>

        <script>
// More info https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  history: true,
  // 是否在右下角展示控制条
  controls: true,
  // 是否显示演示的进度条
  progress: true,
  // 是否显示当前幻灯片的页数编号，也可以使用代码 “slideNumber: 'c/t'” ，表示当前页/总页数。
  slideNumber: false,
  // 是否将每个幻灯片改变加入到浏览器的历史记录中去
  history: false,
  // 是否启用键盘快捷键来导航
  keyboard: true,
  // 是否启用幻灯片的概览模式，可使用 "Esc" 或 "o" 键来切换概览模式
  overview: true,
  // 是否将幻灯片垂直居中
  center: true,
  // 是否在触屏设备上启用触摸滑动切换
  touch: true,
  // 是否循环演示
  loop: false,
  // 是否将演示的方向变成 RTL，即从右往左
  rtl: false,
  // 是否每次演示的时候，随机幻灯片的顺序
  shuffle: false,
  // 全局开启和关闭碎片。
  fragments: true,
  // 标识演示文稿是否在嵌入模式中运行，即包含在屏幕的有限部分中的
  embedded: false,
  // 标识当问号键被点击的时候是否应该显示一个帮助的覆盖层
  help: true,
  // 标识演讲者备注标志是否让所有观看者可见
  showNotes: false,
  // 两个幻灯片之间自动切换的时间间隔（毫秒）
  // 当设置成 0 的时候则禁止自动切换
  // 该值可以被幻灯片上的 “data-autoslide” 属性覆盖
  autoSlide: 0,
  // 当遇到用户输入的时候停止自动切换
  autoSlideStoppable: true,
  // 当自动滑动时，使用此方法进行导航。
  autoSlideMethod: Reveal.navigateNext,
  // 是否启用通过鼠标滚轮来导航幻灯片
  mouseWheel: false,
  // 是否在移动设备上隐藏地址栏
  hideAddressBar: true,
  // 是否在一个弹出的 iframe 中打开幻灯片中的链接
  previewLinks: false,
  // 切换过渡效果
  transition: 'none', // none/fade/slide/convex/concave/zoom
  // 过渡速度
  transitionSpeed: 'default', // default/fast/slow
  // 全屏幻灯片背景的过渡效果
  backgroundTransition: 'none', // none/fade/slide/convex/concave/zoom
  // 加载除当前可见的幻灯片之外的幻灯片数量
  viewDistance: 3,
  // 视差背景图片
  parallaxBackgroundImage: ' ',
  // e.g. 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'
  // 视差背景尺寸
  parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"
  // 移动视差背景（水平和垂直）滑动变化的数量, 例如 100
  // - 除了指定自动计算
  // - 设置为 0 时，禁止沿轴运动
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  width: 1920,
	height: 1200,

	// Factor of the display size that should remain empty around the content
	margin: 0,

	// Bounds for smallest/largest possible scale to apply to content
	minScale: 0.9,
	maxScale: 3,
  // More info https://github.com/hakimel/reveal.js#dependencies
  math: {
		mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
		config: 'TeX-AMS-MML_HTMLorMML'  // See http://docs.mathjax.org/en/latest/config-files.html

	},
	//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML
  dependencies: [
    { src: 'https://cdn.bootcss.com/reveal.js/3.4.1/plugin/markdown/marked.js' },
    { src: 'https://cdn.bootcss.com/reveal.js/3.4.1/plugin/math/math.js', async: true },
    { src: 'https://cdn.bootcss.com/reveal.js/3.4.1/plugin/markdown/markdown.min.js' },
    { src: 'https://cdn.bootcss.com/reveal.js/3.4.1/plugin/notes/notes.min.js', async: true },
    { src: 'https://cdn.bootcss.com/reveal.js/3.4.1/plugin/highlight/highlight.min.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});
        </script>
    </body>
</html>
